{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a81c6d18",
   "metadata": {},
   "source": [
    "# Etapa 3: Avaliação e Análise do Modelo\n",
    "\n",
    "Este notebook realiza a avaliação do modelo de classificação de obstáculos com sensor simulado. Siga as etapas para medir o desempenho, visualizar resultados e analisar os pontos fortes e fracos do modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ba22ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalar bibliotecas necessárias (caso não estejam instaladas)\n",
    "!pip install -q pandas numpy matplotlib seaborn scikit-learn tensorflow\n",
    "\n",
    "# Importar bibliotecas\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import tensorflow as tf  # Altere para torch se estiver usando PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea7eac67",
   "metadata": {},
   "source": [
    "## Carregar Modelo Treinado e Dados de Teste\n",
    "\n",
    "Carregue o modelo treinado salvo na etapa anterior e o conjunto de dados de teste preparado no pré-processamento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ed4a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Substitua os caminhos pelos corretos do seu Google Drive ou ambiente\n",
    "# Carregar modelo treinado (Keras)\n",
    "modelo = tf.keras.models.load_model('/content/drive/MyDrive/seu_modelo.h5')\n",
    "\n",
    "# Carregar dados de teste\n",
    "X_test = np.load('/content/drive/MyDrive/X_test.npy')\n",
    "y_test = np.load('/content/drive/MyDrive/y_test.npy')\n",
    "\n",
    "# Se estiver usando PyTorch, adapte para torch.load e torch.Tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63aeeb21",
   "metadata": {},
   "source": [
    "## Realizar Previsões\n",
    "\n",
    "Utilize o modelo carregado para fazer previsões sobre os dados de teste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b44bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Previsões\n",
    "pred_probs = modelo.predict(X_test)\n",
    "pred_classes = np.argmax(pred_probs, axis=1)\n",
    "true_classes = np.argmax(y_test, axis=1)  # Se y_test estiver one-hot encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa41141",
   "metadata": {},
   "source": [
    "## Calcular Métricas de Avaliação\n",
    "\n",
    "Calcule acurácia, precisão, recall, F1-score e matriz de confusão usando sklearn.metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "849c5843",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Métricas de avaliação\n",
    "print('Acurácia:', accuracy_score(true_classes, pred_classes))\n",
    "print('\\nRelatório de Classificação:\\n', classification_report(true_classes, pred_classes))\n",
    "cm = confusion_matrix(true_classes, pred_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f8aa0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gráfico de barras para métricas por classe\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(true_classes, pred_classes)\n",
    "classes = [classe_labels.get(i, str(i)) for i in range(len(precision))]\n",
    "plt.figure(figsize=(10,5))\n",
    "x = np.arange(len(classes))\n",
    "plt.bar(x-0.2, precision, width=0.2, label='Precision')\n",
    "plt.bar(x, recall, width=0.2, label='Recall')\n",
    "plt.bar(x+0.2, f1, width=0.2, label='F1-score')\n",
    "plt.xticks(x, classes)\n",
    "plt.ylabel('Score')\n",
    "plt.title('Métricas por Classe')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92924a0",
   "metadata": {},
   "source": [
    "## Visualizar Resultados\n",
    "\n",
    "Gere um mapa de calor da matriz de confusão e visualize exemplos de sequências de teste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d00ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualização da matriz de confusão\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel('Predito')\n",
    "plt.ylabel('Real')\n",
    "plt.title('Matriz de Confusão')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c5aee39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar exemplos de sequências de teste com previsões vs rótulos reais\n",
    "# Exemplo: plota as 3 primeiras sequências de erro (ajuste conforme seu formato de X_test)\n",
    "num_exemplos = 3\n",
    "for i, idx in enumerate(erros[:num_exemplos]):\n",
    "    plt.figure(figsize=(10,3))\n",
    "    plt.plot(X_test[idx], label='Sequência de entrada')\n",
    "    plt.title(f'Exemplo {idx} - Real: {true_classes[idx]}, Predito: {pred_classes[idx]}')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f344a443",
   "metadata": {},
   "source": [
    "## Análise dos Erros\n",
    "\n",
    "Identifique os tipos de obstáculos que o modelo teve mais dificuldade em classificar examinando a matriz de confusão e exemplos de erros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2857077b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemplo: mostrar índices onde houve erro de classificação\n",
    "erros = np.where(pred_classes != true_classes)[0]\n",
    "print(f'Número de erros: {len(erros)}')\n",
    "# Exibir alguns exemplos\n",
    "for idx in erros[:5]:\n",
    "    print(f'Exemplo {idx} - Real: {true_classes[idx]}, Predito: {pred_classes[idx]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapeamento de classes (ajuste conforme seu dataset)\n",
    "# Exemplo: {0: 'Pessoa', 1: 'Carro', 2: 'Objeto Pequeno', 3: 'Parede Fixa'}\n",
    "classe_labels = {0: 'Pessoa', 1: 'Carro', 2: 'Objeto Pequeno', 3: 'Parede Fixa'}\n",
    "print('Erros por classe:')\n",
    "from collections import Counter\n",
    "erros_classes = Counter(true_classes[erros])\n",
    "for classe, count in erros_classes.items():\n",
    "    print(f'{classe_labels.get(classe, classe)}: {count} erros')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a133ae",
   "metadata": {},
   "source": [
    "## (Opcional) Análise Textual com Gemini/AI Studio\n",
    "\n",
    "Copie as métricas de avaliação e a matriz de confusão e cole no Google AI Studio. Peça ao Gemini para analisar os resultados e explicar em linguagem natural o desempenho do modelo, principais acertos e erros."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac883481",
   "metadata": {},
   "source": [
    "## Conclusão\n",
    "\n",
    "Documente as principais métricas, visualizações e conclusões sobre o desempenho do modelo. Aponte quais obstáculos são bem classificados, quais são problemáticos e possíveis razões."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
